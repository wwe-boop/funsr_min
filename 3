# FunASR-VAD 语音活动检测算法详细流程

## 概述

FunASR-VAD是基于FSMN（Feedforward Sequential Memory Network）的语音活动检测系统，能够实时检测音频中的语音段和静音段。该算法采用深度学习模型结合传统信号处理技术，实现高精度的语音端点检测。

## 算法整体架构
一些
```
音频输入 → 特征提取 → FSMN模型推理 → 后处理 → 语音段输出
    ↓         ↓           ↓          ↓
  预处理   Fbank特征   VAD分数计算  状态机处理
```

## 详细算法流程

### 1. 音频预处理阶段

#### 1.1 音频加载与标准化
```python
# 音频采样率标准化为16kHz
waveform = load_audio_file(audio_path, sample_rate=16000)

# 幅度标准化（可选）
if upsacle_samples:
    waveform = waveform * (1 << 15)  # 放大到16位整数范围
```

**音频采样率的影响与音频形式转换**：

**原始音频形式**：
- **模拟音频**: 连续的电压信号，幅度随时间连续变化
- **物理特性**: 无限精度的时间和幅度信息

**采样后的数字音频形式**：
- **离散时间信号**: 每隔1/16000秒采样一次，得到离散的数值序列
- **量化精度**: 通常使用16位或32位浮点数表示每个采样点的幅度值
- **数据结构**: `waveform[n] = amplitude_value`，其中n为时间索引

**采样率16kHz的技术意义**：
1. **奈奎斯特定理**: 采样率必须≥2倍最高频率，16kHz可完整重建8kHz以下的信号
   - **奈奎斯特定理详解**: 为了无失真地重建连续信号，采样频率必须至少是信号最高频率的两倍
   - **信号频率含义**: 指音频信号中包含的各种频率成分，语音信号主要集中在300Hz-3400Hz
   - **采样必要性**: 连续的模拟信号包含无限信息，数字系统只能处理离散数据，采样是模数转换的关键步骤
2. **语音频谱覆盖**: 人类语音的主要能量集中在0-4kHz，8kHz已足够覆盖语音信息
3. **计算效率**: 相比44.1kHz音乐采样率，16kHz减少64%的数据量和计算量
4. **标准化**: 语音识别领域的标准采样率，便于模型训练和部署

**幅度标准化的技术含义**：

**标准化目的**：
- **数值范围统一**: 将浮点数幅度值(-1.0到1.0)转换为16位整数范围(-32768到32767)
- **精度保持**: `(1 << 15) = 32768`，最大化利用16位整数的动态范围
- **硬件兼容**: 许多音频处理硬件和算法期望16位整数输入

**数学变换**：
```
原始范围: [-1.0, 1.0] (32位浮点)
变换公式: int16_value = float_value × 32768
目标范围: [-32768, 32767] (16位整数)
```

**实际意义**：
- **信噪比保持**: 保持原始音频的相对幅度关系
- **量化噪声最小**: 充分利用16位精度，减少量化误差
- **处理链兼容**: 确保后续特征提取算法的数值稳定性

#### 1.2 分块处理
对于长音频，采用重叠分块策略：
- **分块大小**: 默认60秒
- **重叠比例**: 10%
- **时间偏移计算**: `time_offset = start_sample * 1000 / sample_rate`

**重叠分块的具体操作与技术意义**：

```python
# 重叠分块示例代码
chunk_size = 60 * 16000  # 60秒 × 16kHz = 960,000 samples
overlap_size = int(chunk_size * 0.1)  # 10%重叠 = 96,000 samples
step_size = chunk_size - overlap_size  # 步进 = 864,000 samples

chunks = []
for i in range(0, len(waveform) - chunk_size + 1, step_size):
    chunk = waveform[i:i + chunk_size]
    chunks.append((chunk, i))  # 保存数据和起始位置
```

**分块操作的数学描述**：
```
时间轴分割:
Chunk 1: [0s,      60s]     → samples [0,        959999]
Chunk 2: [54s,     114s]    → samples [864000,   1823999]
Chunk 3: [108s,    168s]    → samples [1728000,  2687999]
...

重叠区域: 每两个相邻块重叠6秒 (96,000 samples)
步进长度: 54秒 (864,000 samples)
```

**重叠分块的技术意义**：

1. **边界效应消除**：
   - **问题**: 语音事件可能跨越块边界，单独处理会造成信息丢失
   - **解决**: 重叠区域确保完整语音事件至少在一个块中完整出现
   - **实例**: 一个10秒的语音段在58秒边界处开始，重叠处理确保在下一块中完整保留

2. **时间连续性保持**：
   - **平滑过渡**: 重叠区域提供时间上下文信息，避免检测结果突变
   - **上下文保持**: VAD检测需要前后文信息，重叠确保上下文不丢失

3. **检测精度提升**：
   - **多次检测**: 同一语音段在多个块中被处理，提高检测置信度
   - **时间精度**: 10%重叠率将边界区域的时间定位精度显著提升

4. **算法鲁棒性**：
   - **噪声抑制**: 多次检测结果的一致性可以过滤偶然噪声
   - **误检减少**: 真实语音在重叠区域会被多次确认，提高可靠性

### 2. 特征提取阶段

#### 2.1 Fbank特征提取
使用Kaldi工具包提取梅尔频率倒谱系数：

```python
fbank = kaldi.fbank(
    waveform,
    num_mel_bins=80,           # 梅尔滤波器组数量
    frame_length=25.0,         # 帧长度(ms)
    frame_shift=10.0,          # 帧移(ms)
    dither=0.0,               # 抖动系数
    window_type="hamming",     # 窗函数类型
    sample_frequency=16000,    # 采样率
    snip_edges=True           # 边界处理
)
```

**Fbank特征的深度技术解析**：

**什么是Fbank特征**：
Fbank（Filter Bank）是一种基于梅尔频率刻度的音频特征提取方法，专门设计用于语音信号处理。它模拟人类听觉系统的频率感知特性，将线性频率转换为更符合人耳感知的梅尔频率。

**梅尔频率倒谱系数(MFCC)与Fbank的关系**：
- **Fbank**: 梅尔滤波器组的对数输出，保留了频谱的详细信息
- **MFCC**: 在Fbank基础上进行离散余弦变换(DCT)，进一步压缩和去相关
- **VAD应用**: Fbank保留更多频谱细节，更适合语音活动检测任务

**梅尔频率倒谱系数的技术意义**：
1. **感知线性化**: 梅尔刻度更好地反映人类听觉的频率分辨率特性
2. **低频敏感**: 人耳对低频变化更敏感，梅尔刻度在低频区域分辨率更高
3. **计算效率**: 相比全频谱分析，80维Fbank大幅减少计算复杂度
4. **鲁棒性**: 对噪声和通道失真具有更好的抗干扰能力

**Fbank特征计算公式详解**:

1. **短时傅里叶变换(STFT)**:
$$X(k) = \sum_{n=0}^{N-1} x(n) \cdot w(n) \cdot e^{-j2\pi kn/N}$$

参数说明：
- $x(n)$: 时域音频信号，长度N=400个采样点(25ms×16kHz)
- $w(n)$: 汉明窗函数，$w(n) = 0.54 - 0.46\cos(\frac{2\pi n}{N-1})$
- $X(k)$: 第k个频率bin的复数频域表示，k=0,1,...,N/2
- $N$: FFT点数，通常为512点

**时域到频域转换的技术意义**：

**为什么需要时域到频域转换**：
1. **频谱分析**: 语音信号的关键信息主要体现在频域特性中
2. **特征提取**: 频域能够揭示语音的共振峰、基频等重要特征
3. **噪声分离**: 语音和噪声在频域具有不同的分布特性，便于分离
4. **计算效率**: FFT算法提供了高效的频域变换方法

**汉明窗函数的作用机制**：
- **频谱泄漏抑制**: 减少由于信号截断造成的频谱泄漏现象
- **旁瓣抑制**: 汉明窗的频域响应具有较低的旁瓣，减少频率间干扰
- **平滑过渡**: 窗函数边缘的平滑过渡避免了矩形窗的突变效应
- **数学特性**: 汉明窗在主瓣宽度和旁瓣抑制之间提供了良好的平衡

**FFT点数的技术含义**：
- **512点FFT**: 提供256个有效频率bin（奈奎斯特定理）
- **频率分辨率**: 16000Hz/512 = 31.25Hz，每个bin代表31.25Hz的频率范围
- **计算复杂度**: 512点FFT的复杂度为O(N log N) = O(512 × 9) ≈ 4608次运算
- **精度权衡**: 更多FFT点提供更高频率分辨率，但增加计算量

**不同FFT点数的影响**：
```
256点: 频率分辨率62.5Hz，计算快但精度低
512点: 频率分辨率31.25Hz，精度与效率的平衡点
1024点: 频率分辨率15.625Hz，高精度但计算量大
2048点: 频率分辨率7.8Hz，超高精度，适用于音乐分析
```

2. **功率谱计算**:
$$P(k) = |X(k)|^2 = \text{Real}(X(k))^2 + \text{Imag}(X(k))^2$$

3. **梅尔滤波器组输出**:
$$M(m) = \sum_{k=0}^{N/2} P(k) \cdot H_m(k)$$

其中 $H_m(k)$ 是第m个梅尔滤波器的频率响应：
- 梅尔刻度转换: $\text{mel}(f) = 2595 \log_{10}(1 + \frac{f}{700})$
- 80个滤波器均匀分布在梅尔刻度上，覆盖0-8000Hz
- 每个滤波器为三角形窗，相邻滤波器50%重叠

4. **对数变换**:
$$\text{Fbank}(m) = \log(M(m) + \epsilon)$$
其中 $\epsilon = 1e-8$ 防止对数运算中的数值不稳定

#### 2.2 LFR (Low Frame Rate) 处理
对特征进行低帧率处理以减少计算量：

```python
def apply_lfr(inputs, lfr_m, lfr_n):
    # lfr_m: 堆叠帧数, lfr_n: 下采样因子
    T, D = inputs.shape
    T_lfr = math.ceil(T / lfr_n)
    lfr_inputs = torch.zeros(T_lfr, D * lfr_m)

    for i in range(T_lfr):
        for j in range(lfr_m):
            frame_idx = i * lfr_n + j
            if frame_idx < T:
                lfr_inputs[i, j*D:(j+1)*D] = inputs[frame_idx]

    return lfr_inputs
```

**低帧率处理的详细技术解析**：

**LFR处理的核心机制**：
LFR（Low Frame Rate）是一种通过帧堆叠和下采样来减少序列长度的技术，在保持关键信息的同时显著降低计算复杂度。

**具体操作步骤**：
1. **帧堆叠（Frame Stacking）**：
   ```
   原始特征: [frame_0, frame_1, frame_2, frame_3, frame_4, frame_5, ...]
   堆叠操作: [frame_0+frame_1+frame_2+frame_3+frame_4] → 新特征向量
   维度变化: 80维 → 400维 (80 × 5)
   ```

2. **下采样（Downsampling）**：
   ```
   原始时间轴: t=0, t=1, t=2, t=3, t=4, t=5, t=6, t=7, t=8, ...
   下采样后:   t=0,      t=3,      t=6,      t=9, ...
   采样间隔:   每3帧取1帧进行处理
   ```

**数学表示**：
```
输入: X ∈ R^(T×D), T=时间帧数, D=特征维度(80)
输出: Y ∈ R^(T'×D'), T'=⌈T/lfr_n⌉, D'=D×lfr_m

Y[i] = [X[i×lfr_n], X[i×lfr_n+1], ..., X[i×lfr_n+lfr_m-1]]
```

**技术优势分析**：

1. **计算效率提升**：
   - **序列长度减少**: 从100fps降低到33fps，减少67%的时间步
   - **并行计算**: 堆叠后的特征向量可以更好地利用向量化计算
   - **内存优化**: 减少RNN/LSTM的循环计算次数

2. **信息保持机制**：
   - **时序信息**: 5帧堆叠保留了50ms的时序上下文信息
   - **频谱细节**: 所有频谱信息都被保留，只是重新组织
   - **局部相关性**: 相邻帧的相关性通过堆叠得到加强

3. **模型适配性**：
   - **输入维度**: 400维输入更适合深度网络的处理
   - **感受野**: 每个处理单元能"看到"更长的时间窗口
   - **特征丰富度**: 多帧信息融合提供更丰富的特征表示

**参数选择的技术考量**：
- **lfr_m=5**: 堆叠5帧对应50ms时间窗口，覆盖一个音素的典型时长
- **lfr_n=3**: 下采样因子3提供了效率和精度的最佳平衡点
- **压缩比**: 3:1的压缩比在保持性能的同时显著提升效率

#### 2.3 CMVN (Cepstral Mean and Variance Normalization) 详解
对特征进行均值方差归一化以提高模型鲁棒性：

**归一化公式**:
$$\hat{x}_t = \frac{x_t - \mu}{\sigma}$$

**参数计算**:
- $\mu = \frac{1}{T}\sum_{t=1}^{T} x_t$ (全局均值，T为总帧数)
- $\sigma = \sqrt{\frac{1}{T}\sum_{t=1}^{T} (x_t - \mu)^2}$ (全局标准差)

**技术意义**:
- **消除通道差异**: 不同录音设备、环境的影响
- **数值稳定性**: 将特征值标准化到均值0、方差1的分布
- **加速收敛**: 帮助神经网络更快收敛
- **维度独立**: 对80维Fbank特征的每一维独立进行归一化

**实现方式**:
```python
# 全局统计量计算(离线方式)
global_mean = np.mean(all_features, axis=0)  # shape: [80]
global_std = np.std(all_features, axis=0)    # shape: [80]

# 在线归一化
normalized_features = (features - global_mean) / (global_std + 1e-8)
```

### 3. FSMN模型推理阶段

#### 3.1 FSMN网络结构
FSMN由以下组件构成：

1. **输入线性层**: 
   ```python
   x1 = Linear(input_dim=400, output_dim=140)(input)
   x2 = Linear(140, 140)(x1)
   x3 = ReLU()(x2)
   ```

2. **FSMN核心层**:
   ```python
   # 左侧记忆单元
   left_context = Conv1d(
       in_channels=proj_dim,
       out_channels=proj_dim, 
       kernel_size=lorder,
       groups=proj_dim
   )
   
   # 右侧记忆单元（可选）
   right_context = Conv1d(
       in_channels=proj_dim,
       out_channels=proj_dim,
       kernel_size=rorder,
       groups=proj_dim
   )
   ```

3. **输出层**:
   ```python
   x5 = Linear(140, 140)(x4)
   x6 = Linear(140, 2)(x5)  # 2分类：语音/静音
   x7 = Softmax(dim=-1)(x6)
   ```

#### 3.2 FSMN前向传播公式
对于第t帧的输出：

$$h_t = \sigma(W_h \cdot x_t + \sum_{i=1}^{L} W_i \cdot h_{t-i} + \sum_{j=1}^{R} W_j \cdot h_{t+j})$$

其中：
- $x_t$: 第t帧输入特征
- $h_t$: 第t帧隐藏状态
- $W_h, W_i, W_j$: 权重矩阵
- $L, R$: 左右上下文长度
- $\sigma$: 激活函数

#### 3.3 VAD分数计算
模型输出2维概率分布：
```python
scores = model(features)  # [batch, frames, 2]
speech_probs = scores[:, :, 1]  # 语音概率
silence_probs = scores[:, :, 0]  # 静音概率
```

### 4. 后处理阶段

#### 4.1 帧级别判决
根据阈值将概率转换为帧状态：

```python
def frame_decision(speech_prob, threshold=0.5):
    if speech_prob > threshold:
        return FrameState.kFrameStateSpeech
    else:
        return FrameState.kFrameStateSil
```

#### 4.2 滑动窗口平滑
使用滑动窗口减少噪声影响：

```python
class WindowDetector:
    def __init__(self, window_size_ms, frame_size_ms):
        self.win_size_frame = window_size_ms // frame_size_ms
        self.win_state = [0] * self.win_size_frame
        self.win_sum = 0
        self.cur_win_pos = 0
    
    def detect_one_frame(self, frame_state):
        # 更新滑动窗口
        self.win_sum += frame_state.value - self.win_state[self.cur_win_pos]
        self.win_state[self.cur_win_pos] = frame_state.value
        self.cur_win_pos = (self.cur_win_pos + 1) % self.win_size_frame
        
        # 状态转换判决
        return self.state_transition()
```

#### 4.3 状态机处理
实现语音段的起始和结束检测：

**状态定义**:
- `kVadInStateStartPointNotDetected`: 未检测到起始点
- `kVadInStateInSpeechSegment`: 在语音段中
- `kVadInStateEndPointDetected`: 检测到结束点

**状态转换条件**:
```python
# 静音到语音转换
if voice_frame_count >= sil_to_speech_threshold:
    state = kChangeStateSil2Speech
    
# 语音到静音转换  
if silence_frame_count >= speech_to_sil_threshold:
    state = kChangeStateSpeech2Sil
```

#### 4.4 分贝计算详解
计算每帧的能量分贝值用于辅助VAD判决：

**RMS(均方根)计算**:
$$\text{RMS}_t = \sqrt{\frac{1}{N}\sum_{n=1}^{N} x_t^2(n)}$$

**分贝转换**:
$$dB_t = 20 \log_{10}(\text{RMS}_t + \epsilon)$$

**参数说明**:
- $x_t(n)$: 第t帧的第n个采样点，n=1,2,...,N (N=160，对应10ms)
- $\epsilon = 1e-10$: 防止对数运算中的数值错误
- 分贝值范围: 通常在-60dB到0dB之间

**物理意义**:
- **-60dB**: 接近数字静音，背景噪声水平
- **-40dB**: 很轻的语音或环境噪声
- **-20dB**: 正常语音的低能量部分
- **-10dB**: 正常语音的中等能量
- **0dB**: 接近满量程，大声语音或音乐

**在VAD中的应用**:
```python
def energy_based_vad(frame_db, threshold_db=-35):
    """基于能量的辅助VAD判决"""
    if frame_db > threshold_db:
        return True  # 可能是语音
    else:
        return False  # 可能是静音
```

**与神经网络VAD的结合**:
- 神经网络VAD提供主要判决
- 能量VAD作为辅助验证，防止低能量语音被误判
- 双重验证提高整体检测精度

### 5. 输出格式化

#### 5.1 时间戳转换
将帧索引转换为毫秒时间戳：

```python
def frame_to_time(frame_idx, frame_shift_ms):
    return frame_idx * frame_shift_ms

start_time = frame_to_time(start_frame, 10)  # 10ms帧移
end_time = frame_to_time(end_frame, 10)
```

#### 5.2 语音段合并
合并相邻或重叠的语音段：

```python
def merge_segments(segments, tolerance_ms=100):
    merged = []
    for current in sorted(segments):
        if merged and current[0] <= merged[-1][1] + tolerance_ms:
            merged[-1] = [merged[-1][0], max(merged[-1][1], current[1])]
        else:
            merged.append(current)
    return merged
```

## 关键参数配置

| 参数名称 | 默认值 | 说明 | 详细解释 |
|---------|--------|------|----------|
| speech_noise_thres | 0.5 | 语音/静音判决阈值 | 模型输出概率的判决门限。当语音概率>0.5时判定为语音帧，否则为静音帧。较高的阈值会减少误检但可能漏检弱语音 |
| sil_to_speech_time | 200ms | 静音到语音转换时间 | 从静音状态转换到语音状态需要连续检测到的语音帧时长。相当于20帧(10ms帧移)。防止短暂噪声被误判为语音起始 |
| speech_to_sil_time | 700ms | 语音到静音转换时间 | 从语音状态转换到静音状态需要连续检测到的静音帧时长。相当于70帧。避免语音中的短暂停顿导致语音段被错误分割 |
| window_size_ms | 200ms | 滑动窗口大小 | 平滑处理的窗口长度，相当于20帧。用于减少帧级别判决的抖动，提高检测稳定性 |
| max_end_silence_time | 800ms | 最大结束静音时间 | 语音段结束后允许的最大静音时长。超过此时长则确认语音段结束，开始寻找下一个语音段 |

### 特征提取参数详解

| 参数名称 | 数值 | 技术含义 | 影响说明 |
|---------|------|----------|----------|
| num_mel_bins | 80 | 梅尔滤波器组数量 | 决定频谱分辨率。80个滤波器覆盖0-8kHz频率范围，平衡计算复杂度和频谱细节 |
| frame_length | 25.0ms | 分析窗长度 | 对应400个采样点(16kHz)。较长窗口提供更好的频率分辨率，但时间分辨率降低 |
| frame_shift | 10.0ms | 帧移步长 | 对应160个采样点。决定时间分辨率，10ms提供良好的时间精度用于VAD检测 |
| sample_frequency | 16000Hz | 音频采样率 | 标准语音处理采样率，奈奎斯特频率8kHz覆盖语音主要频谱范围 |

### FSMN网络结构参数

| 层级 | 输入维度 | 输出维度 | 功能说明 |
|------|----------|----------|----------|
| 输入层1 | 400 | 140 | LFR处理后的特征维度压缩。400=80(mel bins)×5(lfr_m堆叠帧数) |
| 输入层2 | 140 | 140 | 特征变换层，保持维度不变进行非线性映射 |
| FSMN层 | 140 | 140 | 核心记忆网络层，整合时序上下文信息 |
| 输出层1 | 140 | 140 | 特征整合层 |
| 输出层2 | 140 | 2 | 分类层，输出语音/静音二分类概率 |

### LFR (Low Frame Rate) 参数

| 参数 | 典型值 | 作用机制 |
|------|--------|----------|
| lfr_m | 5 | 堆叠帧数，将连续5帧特征拼接成一个输入向量 |
| lfr_n | 3 | 下采样因子，每3帧取1帧进行处理 |
| 压缩比 | 3:1 | 将原始100fps(帧/秒)降低到33fps，减少计算量60% |

## 性能指标详解

### 1. 实时性能
- **处理延迟**: < 100ms
  - 包含特征提取(~30ms) + 模型推理(~40ms) + 后处理(~20ms)
  - 满足实时语音应用需求(通常要求<150ms)

### 2. 准确性指标
- **F1-score**: > 0.95
  - F1 = 2×(Precision×Recall)/(Precision+Recall)
  - Precision(精确率): 检测为语音的帧中真正是语音的比例
  - Recall(召回率): 真实语音帧中被正确检测的比例
- **误检率(FAR)**: < 2%
  - False Alarm Rate，静音被误判为语音的比例
- **漏检率(FRR)**: < 3%
  - False Rejection Rate，语音被误判为静音的比例

### 3. 鲁棒性表现
- **信噪比适应性**: 在SNR > 5dB环境下保持高精度
- **回声抑制**: 对AEC残留回声具有20dB以上的抑制能力
- **多语种支持**: 支持中文、英文、日文等多种语言

### 4. 计算效率
- **CPU占用**: 单核CPU使用率 < 15%
- **内存占用**: 模型大小约2MB，运行时内存 < 50MB
- **功耗**: 移动设备上功耗增加 < 5%

## 应用场景

- 语音识别预处理
- 音频会议系统
- 语音质量评估
- 音频内容分析

该算法通过深度学习模型与传统信号处理技术的结合，实现了高精度、低延迟的语音活动检测功能。

## 专业术语词汇表

### 信号处理术语
| 术语 | 英文全称 | 中文解释 | 技术细节 |
|------|----------|----------|----------|
| VAD | Voice Activity Detection | 语音活动检测 | 自动识别音频中语音和静音段的技术 |
| FSMN | Feedforward Sequential Memory Network | 前馈序列记忆网络 | 一种循环神经网络变体，专门处理序列数据 |
| Fbank | Filter Bank | 滤波器组特征 | 基于梅尔刻度的频谱特征，常用于语音识别 |
| STFT | Short-Time Fourier Transform | 短时傅里叶变换 | 分析非平稳信号频谱随时间变化的方法 |
| CMVN | Cepstral Mean and Variance Normalization | 倒谱均值方差归一化 | 消除通道和环境影响的特征归一化技术 |
| LFR | Low Frame Rate | 低帧率处理 | 通过帧堆叠和下采样减少计算量的技术 |
| RMS | Root Mean Square | 均方根 | 信号能量的度量方式，用于计算音频功率 |

### 性能评估术语
| 术语 | 英文全称 | 计算公式 | 意义说明 |
|------|----------|----------|----------|
| Precision | Precision | TP/(TP+FP) | 精确率，预测为正例中真正为正例的比例 |
| Recall | Recall/Sensitivity | TP/(TP+FN) | 召回率，真正例中被正确预测的比例 |
| F1-Score | F1-Score | 2×P×R/(P+R) | 精确率和召回率的调和平均数 |
| FAR | False Alarm Rate | FP/(FP+TN) | 误检率，静音被误判为语音的比例 |
| FRR | False Rejection Rate | FN/(TP+FN) | 漏检率，语音被误判为静音的比例 |
| SNR | Signal-to-Noise Ratio | 10×log10(Ps/Pn) | 信噪比，信号功率与噪声功率的比值 |

### 网络结构术语
| 术语 | 英文全称 | 中文解释 | 在FSMN中的作用 |
|------|----------|----------|----------------|
| Feedforward | Feedforward | 前馈 | 信息单向传播，从输入到输出 |
| Memory Block | Memory Block | 记忆块 | 存储历史信息，提供时序上下文 |
| Context Window | Context Window | 上下文窗口 | 模型能够"看到"的历史和未来帧数 |
| Projection | Projection | 投影 | 维度变换，通常用于降维或升维 |
| Activation | Activation Function | 激活函数 | 引入非线性，常用ReLU、Sigmoid等 |

### 音频处理术语
| 术语 | 单位 | 典型值 | 技术含义 |
|------|------|--------|----------|
| 采样率 | Hz | 16000 | 每秒采样点数，决定音频质量上限 |
| 帧长 | ms | 25 | 分析窗口长度，影响频率分辨率 |
| 帧移 | ms | 10 | 相邻帧的时间间隔，影响时间分辨率 |
| 频率分辨率 | Hz | 62.5 | 相邻频率bin的间隔，16000/256=62.5Hz |
| 时间分辨率 | ms | 10 | 最小可分辨的时间间隔 |
| 动态范围 | dB | 96 | 16位音频的理论动态范围 |

### 算法参数术语
| 参数类型 | 参数名 | 作用机制 | 调优建议 |
|----------|--------|----------|----------|
| 阈值参数 | speech_noise_thres | 概率判决门限 | 噪声环境下可适当提高(0.6-0.7) |
| 时间参数 | sil_to_speech_time | 状态转换延迟 | 快速响应场景可降低到100ms |
| 时间参数 | speech_to_sil_time | 防止误分割 | 连续语音场景可增加到1000ms |
| 窗口参数 | window_size_ms | 平滑程度 | 噪声环境下可增加到300ms |
| 容忍参数 | max_end_silence_time | 段落分割 | 对话场景可增加到1500ms |

这些专业术语和参数的深入理解有助于：
1. **算法调优**: 根据具体应用场景调整参数
2. **性能分析**: 准确评估算法在不同条件下的表现
3. **问题诊断**: 快速定位和解决检测精度问题
4. **系统集成**: 与其他语音处理模块的有效对接
